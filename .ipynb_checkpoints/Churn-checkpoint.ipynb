{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "schema = StructType([ \\\n",
    "    StructField(\"state\", StringType(), True), \\\n",
    "    StructField(\"account_length\", DoubleType(), True), \\\n",
    "    StructField(\"area_code\", StringType(), True), \\\n",
    "    StructField(\"phone_number\", StringType(), True), \\\n",
    "    StructField(\"intl_plan\", StringType(), True), \\\n",
    "    StructField(\"voice_mail_plan\", StringType(), True), \\\n",
    "    StructField(\"number_vmail_messages\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_charge\", DoubleType(), True), \\\n",
    "    StructField(\"number_customer_service_calls\", DoubleType(), True), \\\n",
    "    StructField(\"churned\", StringType(), True)])\n",
    "\n",
    "churn_data = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .load('data/churn.csv', schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = churn_data.count()\n",
    "voice_mail_plans = churn_data.filter(churn_data.voice_mail_plan == \" yes\").count()\n",
    "print('Count: {0}, voice_mail_plans: {1}'.format(count,voice_mail_plans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupBy Aggregation, Join, Column manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states=churn_data.groupBy(\"state\").count()\n",
    "states_vm=churn_data.filter(churn_data.voice_mail_plan == \" yes\").groupBy(\"state\") \\\n",
    "    .count().withColumnRenamed('count', 'count_vm')\n",
    "joined_df = states.join(states_vm, states.state == states_vm.state, 'inner').drop(states_vm.state)\n",
    "df=joined_df.select(joined_df['state'],(joined_df['count_vm']/joined_df['count']).alias('vmp'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from Spark DataFrame to Pandas DataFrame. \n",
    "[df.sample(withReplacement, fraction, seed=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_data = churn_data.sample(False, 0.5, 83).toPandas()\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame.hist(data=sample_data,column='number_customer_service_calls')\n",
    "sb.distplot(sample_data['number_customer_service_calls'], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.boxplot(x=\"churned\", y=\"number_customer_service_calls\", data=sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating columns by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"account_length\", \"number_vmail_messages\", \"total_day_minutes\",\n",
    "                \"total_day_calls\", \"total_day_charge\", \"total_eve_minutes\",\n",
    "                \"total_eve_calls\", \"total_eve_charge\", \"total_night_minutes\",\n",
    "                \"total_night_calls\", \"total_intl_minutes\", \"total_intl_calls\",\n",
    "                \"total_intl_charge\"]\n",
    "reduced_numeric_cols = [\"account_length\", \"number_vmail_messages\", \"total_day_calls\",\n",
    "                        \"total_day_charge\", \"total_eve_calls\", \"total_eve_charge\",\n",
    "                        \"total_night_calls\", \"total_intl_calls\", \"total_intl_charge\"]\n",
    "categorical_cols = [\"state\", \"international_plan\", \"voice_mail_plan\", \"area_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "label_indexer = StringIndexer(inputCol = 'churned', outputCol = 'label')\n",
    "plan_indexer = StringIndexer(inputCol = 'intl_plan', outputCol = 'intl_plan_indexed')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = ['intl_plan_indexed'] + reduced_numeric_cols,\n",
    "    outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(labelCol = 'label', featuresCol = 'features')\n",
    "\n",
    "pipeline = Pipeline(stages=[plan_indexer, label_indexer, assembler, classifier])\n",
    "\n",
    "(train, test) = churn_data.randomSplit([0.7, 0.3])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "print('The AUROC is {0} and the AUPR is {1}'.format(auroc, aupr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(labelCol = 'label', featuresCol = 'features', numTrees=500, seed=201)\n",
    "pipeline = Pipeline(stages=[plan_indexer, label_indexer, assembler, classifier])\n",
    "#(train, test) = churn_data.randomSplit([0.7, 0.3])\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auroc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "print('The AUROC is {0} and the AUPR is {1}'.format(auroc, aupr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numTrees=20: The AUROC is 0.8034334855914316 and the AUPR is 0.6880744080071193"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
